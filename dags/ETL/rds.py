# -*- coding: utf-8 -*-
"""RDS_modified.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J4mlxv0wucZayKxlc_1EqEE7IuoYsmwL
"""

# pip install pandas psycopg2 boto3

import pandas as pd
import boto3
from io import StringIO
import psycopg2
from sqlalchemy import create_engine

class RDSDataUploader:
    def __init__(self, db_params):
        self.db_params = db_params

    def upload_to_rds(self, dataframe, table_name, connection):
        buffer = StringIO()
        dataframe.to_csv(buffer, index=False, header=False)
        buffer.seek(0)

        cursor = connection.cursor()
        cursor.copy_from(buffer, table_name, sep=',')
        connection.commit()
        cursor.close()

class RDSQueryExecutor:
    def __init__(self, db_params):
        self.db_params = db_params

    def perform_queries(self, connection, queries):
        for query in queries:
            try:
                with connection.cursor() as cursor:
                    cursor.execute(query)
                    result = cursor.fetchall()
                    print(result)
            except Exception as e:
                print(f"Error executing query: {e}")

class RDSDataInitializer:
    def __init__(self, db_params):
        self.db_params = db_params

    def create_tables(self, connection, table_creation_queries):
        cursor = connection.cursor()
        for query in table_creation_queries:
            cursor.execute(query)
        connection.commit()
        cursor.close()

    def create_tables(self, connection, file_name, table_name):
        # Add your logic to fetch the appropriate create table query based on file_name and table_name
        create_table_query = get_create_table_query(file_name, table_name)

        if create_table_query:
            cursor = connection.cursor()
            cursor.execute(create_table_query)
            connection.commit()
            cursor.close()

def get_create_table_query(file_name, table_name):
    # Add your logic here to return the create table query based on file_name and table_name
    # For example, you can have a dictionary mapping file_name to create table query
    create_table_queries = {
        'dim_animals.csv': '''drop table if exists public.dim_animals;
                              CREATE TABLE IF NOT EXISTS dim_animals (
                                animal_id VARCHAR(7) PRIMARY KEY,
                                name VARCHAR,
                                dob DATE,
                                sex VARCHAR(1),
                                animal_type VARCHAR NOT NULL,
                                breed VARCHAR,
                                color VARCHAR
                                );''',

        'dim_dates.csv': '''drop table if exists public.dim_dates;
                            CREATE TABLE IF NOT EXISTS dim_dates (
                              date_id VARCHAR(8) PRIMARY KEY,
                              date DATE NOT NULL,
                              year INT2  NOT NULL,
                              month INT2  NOT NULL,
                              day INT2  NOT NULL
                              );''',

        'dim_outcome_types.csv': '''drop table if exists public.dim_outcome_types;
                                    CREATE TABLE IF NOT EXISTS dim_outcome_types (
                                      outcome_type VARCHAR NOT NULL,
                                      outcome_type_id INT PRIMARY KEY
                                      );''',

        'fct_outcomes.csv': '''drop table if exists public.fct_outcomes;
                               CREATE TABLE IF NOT EXISTS fct_outcomes (
                                  outcome_id SERIAL PRIMARY KEY,
                                  animal_id VARCHAR(7) NOT NULL,
                                  date_id VARCHAR(8) NOT NULL,
                                  time TIME NOT NULL,
                                  outcome_type_id INT NOT NULL,
                                  outcome_subtype VARCHAR,
                                  is_fixed BOOL,
                                  FOREIGN KEY (animal_id) REFERENCES dim_animals(animal_id),
                                  FOREIGN KEY (date_id) REFERENCES dim_dates(date_id),
                                  FOREIGN KEY (outcome_type_id) REFERENCES dim_outcome_types(outcome_type_id)
                              );'''
    }

    return create_table_queries.get(file_name)

def read_csv_from_s3(aws_access_key_id, aws_secret_access_key, bucket_name, file_path):
    s3 = boto3.client('s3', aws_access_key_id=aws_access_key_id, aws_secret_access_key=aws_secret_access_key)
    obj = s3.get_object(Bucket=bucket_name, Key=file_path)
    return pd.read_csv(obj['Body'])

def rds_main(file_name, table_name):
    aws_access_key_id = 'AKIAUMKD4AUURZD7A66G'
    aws_secret_access_key = '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'
    bucket_name = 'dcsctransformdata'

    db_params = {
        'dbname': 'postgres',
        'user': 'postgres',
        'password': 'pgadmin123',
        'host': 'datacenterdb.chhu076wp22u.us-east-1.rds.amazonaws.com',
        'port': 5432,
    }

    rds_uploader = RDSDataUploader(db_params)
    rds_query_executor = RDSQueryExecutor(db_params)
    rds_data_initializer = RDSDataInitializer(db_params)

    connection = None
    engine = None
    try:
        connection = psycopg2.connect(**db_params)

        # Create tables if running for the first time
        rds_data_initializer.create_tables(connection, file_name, table_name)

        # Upload data to RDS
        dataframe = read_csv_from_s3(aws_access_key_id, aws_secret_access_key, bucket_name, file_name)
        rds_uploader.upload_to_rds(dataframe, table_name, connection)

        # Create SQLAlchemy engine
        engine = create_engine(
            f"postgresql+psycopg2://{db_params['user']}:{db_params['password']}@{db_params['host']}:{db_params['port']}/{db_params['dbname']}"
        )

        # Sample queries
        sample_queries = [
            f'SELECT * FROM {table_name} LIMIT 5;'
        ]

        rds_query_executor.perform_queries(connection, sample_queries)

    except Exception as e:
        print(f"Error: {e}")

    finally:
        if connection:
            connection.close()
        if engine:
            engine.dispose()